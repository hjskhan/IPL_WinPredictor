# -*- coding: utf-8 -*-
"""IPL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YT0fT9z9O5E3D0pq6Ehq8DIGPuUogEKh
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

deliveries_df = pd.read_csv('/content/drive/MyDrive/IPL/deliveries.csv')
matches_df = pd.read_csv('/content/drive/MyDrive/IPL/matches.csv')

deliveries_df.head(3)

deliveries_df.isna().sum()

deliveries_df.isna().sum().sum()

matches_df.head(3)

matches_df.isna().sum()

matches_df.drop(['umpire3'],axis=1,inplace=True)

matches_df[matches_df.isna().sum()[matches_df.isna().sum()>0].index.tolist()]

'''
Points to be noted:
> The winner is Nan only for those matches when there is draw
> The Player of the match  is Nan only for those matches when there is draw
> City Nan values is Dubai
> Umpire 1 and Umpire 2 are unknown for some matches
'''
matches_df[matches_df.isna().any(axis=1)]

# Filling Nans with 'Dubai' in 'city' column
matches_df['city'].fillna(value='Dubai',inplace=True)

# Filling Nans with 'No-one' in 'player_of_match' column
matches_df['player_of_match'].fillna(value='No-one',inplace=True)

# Filling Nans with 'Draw' in 'winner' column
matches_df['winner'].fillna(value='Draw',inplace=True)

# Filling Nans of 'umpire1' and 'umpire2' with 'ffill' method
matches_df['umpire1'].fillna(value='Aleem Dar',inplace=True)
matches_df['umpire2'].fillna(value='Aleem Dar',inplace=True)
matches_df.isna().sum().sum()

# List of teams who played frequently in almost all seasons
teams = [
    'Royal Challengers Bangalore', 'Kolkata Knight Riders', 'Kings XI Punjab', 'Sunrisers Hyderabad',
    'Mumbai Indians', 'Rajasthan Royals', 'Chennai Super Kings','Delhi Capitals'
]

matches_df['team1'] = matches_df['team1'].str.replace('Deccan Chargers','Sunrisers Hyderabad')
matches_df['team1'] = matches_df['team1'].str.replace('Delhi Daredevils','Delhi Capitals')
matches_df['team2'] = matches_df['team2'].str.replace('Deccan Chargers','Sunrisers Hyderabad')
matches_df['team2'] = matches_df['team2'].str.replace('Delhi Daredevils','Delhi Capitals')
matches_df['winner'] = matches_df['winner'].str.replace('Deccan Chargers','Sunrisers Hyderabad')
matches_df['winner'] = matches_df['winner'].str.replace('Delhi Daredevils','Delhi Capitals')

deliveries_df['batting_team'] = deliveries_df['batting_team'].str.replace('Deccan Chargers','Sunrisers Hyderabad')
deliveries_df['batting_team'] = deliveries_df['batting_team'].str.replace('Delhi Daredevils','Delhi Capitals')
deliveries_df['bowling_team'] = deliveries_df['bowling_team'].str.replace('Deccan Chargers','Sunrisers Hyderabad')
deliveries_df['bowling_team'] = deliveries_df['bowling_team'].str.replace('Delhi Daredevils','Delhi Capitals')

# discarding the matches for the non-frequent teams
x = matches_df['team1'].isin(teams) & matches_df['team2'].isin(teams)
matches_df = matches_df.loc[x[x].index]

x = deliveries_df['match_id'].isin(matches_df['id'].values)
deliveries_df = deliveries_df.loc[x[x].index]

deliveries_df['batting_team'].unique()

# Evaluating the cumulative runs by ball for each inning and match
deliveries_df['runs_cumsum']=deliveries_df.groupby(['match_id','inning']).cumsum()['total_runs'].values.tolist()

# For each over there is 6 balls, therefore limiting the official ball count per over to 6
deliveries_df.loc[deliveries_df['ball'][deliveries_df['ball']>6].index.tolist(),'ball']  = 6

# Evaluating the Current run rate
'''
Current Run Rate = (Total Runs) / (Number 0f Overs + (Number of Balls/6))
'''
deliveries_df['curr_run_rate'] =  deliveries_df['runs_cumsum']/((deliveries_df['over']-1)+deliveries_df['ball']/6)

# total no. of balls Delivered
deliveries_df['total_balls']=(deliveries_df['over']-1)*6 + deliveries_df['ball']

# Total no. of balls left
deliveries_df['balls_left'] = 120 - deliveries_df['total_balls']

# Working out for wickets fall
not_out_ind = deliveries_df[deliveries_df['player_dismissed'].isna()].index.tolist()
out_ind = deliveries_df.loc[~deliveries_df['player_dismissed'].index.isin(not_out_ind),'player_dismissed'].index.tolist()
# for the not out ball index we give it 0 value
deliveries_df.loc[not_out_ind,'wicket_fall'] = 0
# for the out ball index we give it 1 value
deliveries_df.loc[out_ind,'wicket_fall'] = 1
# finally we evaluate the total wickets fell down till ball for eavh innings
deliveries_df['wicket_fall'] = deliveries_df.groupby(['match_id','inning']).cumsum()['wicket_fall'].values.tolist()

deliveries_df.loc[not_out_ind,'player_dismissed'] = 'Not Out'

# Labelling 'Out' and 'Not Out'
x = deliveries_df['player_dismissed'] == 'Not Out'
deliveries_df.loc[x[x].index,'is_wicket'] = 0
x = deliveries_df['player_dismissed'] != 'Not Out'
deliveries_df.loc[x[x].index,'is_wicket'] = 1

# All time leading runs scorer
plt.figure(figsize=(8,6))
x = pd.Series(deliveries_df.groupby('batsman').sum()['total_runs']).nlargest(5)
labels = x.values
colors = sns.color_palette('pastel')
ax = x.plot(kind='bar',color=colors)
plt.title('All time Leading Runs by batsman',fontsize=18)
plt.ylabel('Runs',fontsize=15)
plt.xlabel('Batsman',fontsize=15)
plt.xticks(rotation=45,ha='right',fontsize=13)
plt.bar_label(ax.containers[0],labels=labels)
plt.show()

# All time leading wickets taker
plt.figure(figsize=(8,6))
x = pd.Series(deliveries_df.groupby('bowler').sum()['is_wicket']).nlargest(5)
labels = x.values
colors = sns.color_palette('deep')
ax = x.plot(kind='bar',color=colors)
plt.title('All time Leading Wicket taking Bowler',fontsize=18)
plt.ylabel('Wickets',fontsize=15)
plt.xlabel('Bowler',fontsize=15)
plt.xticks(rotation=45, ha='right', fontsize=12)
plt.bar_label(ax.containers[0],labels=labels)
plt.show()

x = matches_df['winner'].value_counts()
labels= x.values
colors = sns.color_palette(palette='deep')
plt.figure(figsize=(10,6))
ax = x.plot(kind='bar',color=colors)
plt.title('Most Wins by Teams',fontsize=18)
plt.xlabel('Teams',fontsize=15)
plt.ylabel('No, of Wins',fontsize=15)
plt.xticks(rotation=45,ha='right',fontsize=13)
plt.bar_label(ax.containers[0],labels=labels)
plt.show()

# Top 'win by runs'
matches_df[['winner','win_by_runs']].nlargest(columns=['win_by_runs'],n=5)

# Player hitting Most Sixes
x = deliveries_df['total_runs'] == 6
x = pd.Series(deliveries_df.loc[x[x].index,'batsman'].value_counts()).nlargest(10)
labels = x.values
colors = sns.color_palette(palette='deep')
plt.figure(figsize = (10,6))
ax = x.plot(kind='bar',color = colors)
plt.title('Most Sixes by player',fontsize=18)
plt.xlabel('Player Name',fontsize=15)
plt.ylabel('No. of Sixes',fontsize=15)
plt.bar_label(ax.containers[0],labels=labels)
plt.xticks(rotation=45,fontsize=13)
plt.show()

"""#### Now, We prepare a DataFrame to feed in the model fro predictions"""

final_df = deliveries_df[['match_id', 'inning', 'batting_team', 'bowling_team', 'over', 'ball','total_runs', 'runs_cumsum', 'curr_run_rate',
       'total_balls', 'balls_left', 'wicket_fall']]

# Evaluating the target given by the batting team
x = deliveries_df.groupby(['match_id','inning']).sum().reset_index()
target = x[x['inning']==1][['match_id','total_runs']]
final_df = pd.merge(left=final_df,right=target,on='match_id')

'''
Since, win predictions comes into play only in the second inning
Therefore, we shall consider 2nd innings from now onwards
'''
final_df = final_df[final_df['inning']==2]

# Evaluating the run left and the wickets left
final_df['runs_left'] = final_df['total_runs_y'] - final_df['runs_cumsum']
final_df['wickets_left'] = 10 - final_df['wicket_fall']

final_df = final_df.reset_index().drop('index',axis=1)

'''
For a team scoring more than required the Reqd. runs may become Negative
> To fix it, we adjust the minimum value of runs left to zero
'''
x = final_df['runs_left']<=0
final_df.loc[x[x].index.tolist(),['runs_left']] = 0

# Evaluating the Required Run Rate (RRR)
final_df['Req_run_rate'] = (final_df['runs_left'] * 6)/(final_df['balls_left'])
'''
Since, for zero runs reqd; balls left can be zero therefore the RRR might become Nans
> To fix this issue we fill the Nans of RRR with the previous observation
'''

final_df.isna().sum()[final_df.isna().sum()>0]

# Filling the Nans created above
final_df['Req_run_rate'].fillna(method='ffill',inplace=True)

x = final_df['Req_run_rate'] == np.inf
ind = x[x].index.tolist()
for i in ind:
    final_df.loc[i,'Req_run_rate'] = final_df.loc[i - 1,'Req_run_rate']

# Renaming some columns
final_df.rename(columns={'total_runs_x':'run_scored','total_runs_y':'target'},inplace=True)

'''
Now, finding our target variable i.e the winner of the match
'''
winner = matches_df[['id','winner']]
final_df = pd.merge(left=final_df,right=winner,right_on='id',left_on='match_id')
final_df.drop('id',axis=1,inplace=True)

x = final_df['winner'] == 'Draw'
ind = x[x].index.tolist()
final_df.drop(index=ind,axis=0,inplace=True)

# If the batting wins in second innings, we label it '1'
x = final_df['batting_team'] == final_df['winner']
final_df.loc[x[x].index.tolist(),'winner'] = 1
# If bowling team wins in second inning, we label it '0'
x = final_df['bowling_team'] == final_df['winner']
final_df.loc[x[x].index.tolist(),'winner'] = 0

final_df.head(3)

# LabelEncoding the Data
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
final_df['batting_team'] = le.fit_transform(final_df['batting_team'])
final_df['bowling_team'] = le.fit_transform(final_df['bowling_team'])

"""##### Feature Selection

###### One thing to note while selecting features is that, apart from feature selection based on statistcal values we may select some features which make basic sense and may drop those features which may not be useful in practical life.
"""

#features = ['batting_team', 'bowling_team','balls_left',  'target', 'runs_left', 'wickets_left']
# We select the 'winner' as the target variable.
Y = final_df['winner']
# Now
X = final_df[[i for i in final_df.columns if i not in  ['winner','match_id','inning']]]

Y = Y.astype(int)
Y.info()

plt.figure(figsize=(20,15))
sns.heatmap( X.corr(),cmap='RdBu',annot=True,center=0)
plt.show()

"""<b>Based on above corelation matrix we delete the highly corelated features:
    
    'over','runs_cumsum','total_balls','balls_left','wicket_fall'

<br><b>Also, the 'ball' variable defines the n'th ball of the over. Since, it is of no use practically, therefore we drop it.</b>

#### <b>Features finalized</b>:
     'batting_team','bowling_team','run_scored','curr_run_rate','target','wickets_left', 'Req_run_rate','runs_left'
"""

X = final_df[[ 'batting_team','bowling_team','run_scored','curr_run_rate',
              'target','wickets_left', 'Req_run_rate','runs_left']]

# Finally we can see that highly corelated variables are dropped
plt.figure(figsize = (8,5))
sns.heatmap(X.corr(),cmap='RdBu',center=0,annot=True)

# Splitting Data into Training set and Test set
train_X = X.sample(frac=0.9,random_state=0)
train_Y = Y.loc[train_X.index]
test_X = X.drop(train_X.index)
test_Y = Y.drop(train_X.index)

# Saving the mean and Std. Deviation of each columns for future use
import pickle
scaler_data = pd.DataFrame({'Mean':train_X.mean().values,'SD':train_X.std().values},
                          index=train_X.columns)
pickle.dump(scaler_data,open('scaler_data.pkl','wb'))

# Scaling the Data for better fit
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
train_X = pd.DataFrame(scaler.fit_transform(train_X),columns=train_X.columns,index=train_X.index)
scaler = StandardScaler()
test_X = pd.DataFrame(scaler.fit_transform(test_X),columns=test_X.columns,index=test_X.index)

# Fitting the model on K-fold CV for Train set

from sklearn.model_selection import train_test_split,GridSearchCV,KFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import mean_squared_error,accuracy_score

k = 5
kf = KFold(n_splits=k, shuffle=True, random_state=42)

rf = RandomForestClassifier(random_state=0)
lg = LogisticRegression(random_state=0,max_iter=5000)
dt = DecisionTreeClassifier()
knn = KNeighborsClassifier()


model=[rf,lg,dt,knn]
accuracy_scores = []
mean_acc=[]
std_acc=[]
rmse_scores=[]
mean_rmse=[]
# Perform k-fold cross-validation
for i in model:
    for train_index, test_index in kf.split(train_X):
        X_train, X_test = train_X.iloc[train_index], train_X.iloc[test_index]
        Y_train, Y_test = train_Y.iloc[train_index], train_Y.iloc[test_index]

        i.fit(X_train, Y_train)
        Y_pred = i.predict(X_test)
        # Calculate and store the accuracy for this fold
        accuracy = accuracy_score(Y_test, Y_pred)
        rmse = np.sqrt(mean_squared_error(Y_test, Y_pred))
        accuracy_scores.append(accuracy)
        rmse_scores.append(rmse)
    mean_acc.append(np.mean(accuracy_scores))
    mean_rmse.append(np.mean(rmse))

train_model_CV = pd.DataFrame({'RMSE_CV':mean_rmse,'Accuracy_CV':mean_acc},
                              index=['RandomForest','LogisticRegression','DescisionTree','KNN'])

# Testing the model
from sklearn.model_selection import train_test_split,GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import mean_squared_error,accuracy_score,confusion_matrix,classification_report

model=[rf,lg,dt,knn]
RMSE=[]
score=[]
acc = []
for i in model:
    Y_predict2 = i.predict(test_X)
    RMSE.append(np.sqrt(mean_squared_error(test_Y,Y_predict2)))
    score.append(i.score(test_X,test_Y))

test_model = pd.DataFrame({'RMSE_test':RMSE,'Accuracy_test':score},
                          index=['RandomForest','LogisticRegression','DescisionTree','KNN'])

# Model Comparision Between Train Models and Test Models
model_metrics = train_model_CV.join(test_model)
model_metrics

import matplotlib.pyplot as plt
colors = sns.color_palette(palette='deep')
ax = model_metrics.plot(kind='bar', figsize=(15, 8), color=colors, width=0.8)
plt.title('Comparison of Models for trained and tested', fontsize=20, fontweight='bold')
plt.ylabel('Model Metrics', fontsize=15)
plt.xticks(rotation=0, fontsize=15)
plt.legend(bbox_to_anchor=(1, 0.5), loc='upper left', fontsize=15)
for container in ax.containers:
    ax.bar_label(container, labels=[round(h,3) for h in container.datavalues],
                 label_type='edge', fontsize=12,fontweight='bold')
plt.show()

"""We may try once for the ANN Model also, as it may give low error value for increased accuracy"""

from keras.models import Sequential
from keras.layers import Dense,Dropout
from keras.optimizers import Adam
from keras.losses import BinaryCrossentropy
from keras.callbacks import EarlyStopping

model = Sequential()
model.add(Dense(units=2048, activation='relu', input_dim=X_train.shape[1]))
model.add(Dropout(0.2))
model.add(Dense(units=1024, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(units=512, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(units=256, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(units=128, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(units=32, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(units=1, activation='sigmoid'))

model.compile(optimizer=Adam(learning_rate=0.0001), loss=BinaryCrossentropy(), metrics=['accuracy'])

early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)

epochs = 100  # You can adjust the number of epochs based on your dataset and convergence
batch_size = 32  # You can adjust the batch size as well
history = model.fit(train_X, train_Y, epochs=epochs, batch_size=batch_size,
                    validation_split=0.1, callbacks=[early_stopping])

loss, accuracy = model.evaluate(X_test, Y_test)
print("Test Loss:", loss)
print("Test Accuracy:", accuracy)

plt.figure(figsize=(8, 6))
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Metric')
plt.title('Accuracy and Loss at Each Epoch')
plt.legend()
plt.show()

# Testing the ANN model
Y_predict = model.predict(test_X)
print(np.sqrt(mean_squared_error(test_Y,Y_predict)))
model.evaluate(test_X,test_Y)

"""##### HyperParameter Tuning
We did HyperParameter Tuning for the ANN model and we ended up with the above configuration of the model

<br></br>
> ANN Model with 256 as maximum number of nodes<br>
> We see that tere is irregularity in loss and accuracy for test data.
> The model has low steepness, this may take large number of epochs to reach better result.
<br></br>
![ANN Model with 256 as maximum number of nodes](https://drive.google.com/uc?export=view&id=1ToODUTF2bLjKSLLMnygswHdIlZuilual)
<br></br>

> ANN Model with 512 as maximum number of nodes<br>
> Still the problem continues.
<br></br>
![ANN Model with 512 as maximum number of nodes](https://drive.google.com/uc?export=view&id=13TropnjTzj9Oyfircev0f5xthj9pDr7X)

> ANN Model with 1024 as maximum number of nodes<br>
> Still the problem continues.
<br></br>
![ANN Model with 1024 as maximum number of nodes](https://drive.google.com/uc?export=view&id=1q1uUHygNoUC_CxVjW4WfwY6BUhzB-84s)
<br></br>
> ANN Model with 2048 as maximum number of nodes<br>
![ANN Model with 2048 as maximum number of nodes](https://drive.google.com/uc?export=view&id=1JrmE4JzxGC3DHiS5PbrvD4hP0RJDXL-v)
<br></br>
> ANN Model with 2048 as maximum number of nodes and some Dropouts 0f 0.2<br>
> To overcome overfitting we introduce dropouts.<br>
> This model gives best result and least value of loss.
<br></br>
![ANN Model with 2048 and some Dropouts as maximum number of nodes](https://drive.google.com/uc?export=view&id=19BrWDtSK-31zMizig_94b7R8C3EfM94E)
<br></br>
<br></br>
####### We tried with other models also, but the above model gave the most stable values. Therefore, we finalize the above model.<br> Now in comprision to the conventional models such as random forest, logistic regression...etc. the ANN model gives least values of loss and maximum accuracy.


"""

import pickle
pickle.dump(model,open('model_ann2048_dp.pkl','wb'))

